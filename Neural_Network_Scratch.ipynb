{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "zvUKwoB80rQe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data, assign test and training set\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
      ],
      "metadata": {
        "id": "LQ7zkjoIhQxW"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encode Vector\n",
        "\n",
        "def one_hot_encode(y, num_classes):\n",
        "    if y.ndim > 1 and y.shape[1] == num_classes: # Already one-hot encoded\n",
        "\n",
        "        return y\n",
        "    y = y.astype(int)\n",
        "    one_hot = np.zeros((y.shape[0], num_classes))\n",
        "    one_hot[np.arange(y.shape[0]), y] = 1\n",
        "    return one_hot\n",
        "\n",
        "x_train = x_train.reshape(-1, 28*28) / 255.0\n",
        "x_test = x_test.reshape(-1, 28*28) / 255.0\n",
        "\n",
        "y_train = one_hot_encode(y_train, 10)\n",
        "y_test = one_hot_encode(y_test, 10)\n",
        "\n",
        "\n",
        "print(f\"Training samples: {x_train.shape[0]}, Test samples: {x_test.shape[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSiC3FefhZky",
        "outputId": "75468664-2423-4e66-8541-9898044259d5"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 60000, Test samples: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialization and Forward Propagation set up\n",
        "\n",
        "def initialize_parameters(input_size, hidden_size, output_size):\n",
        "    W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "    b1 = np.zeros((1, hidden_size))\n",
        "\n",
        "    W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "    b2 = np.zeros((1, output_size))\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def ReLU(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def softmax(Z):\n",
        "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
        "\n",
        "def forward_prop(X, W1, b1, W2, b2):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = softmax(Z2)\n",
        "\n",
        "    return A1, A2\n",
        "\n"
      ],
      "metadata": {
        "id": "hyP8vYBmhce1"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Entropy Loss\n",
        "\n",
        "def compute_loss(A2, Y):\n",
        "    m = Y.shape[0]\n",
        "    loss = -np.sum(Y * np.log(A2)) / m\n",
        "    return loss\n",
        "\n",
        "# Backward Propagation\n",
        "\n",
        "def backward_prop(X, Y, A1, A2, W1, W2):\n",
        "    n = X.shape[0]\n",
        "\n",
        "    # Output layer (Layer 2)\n",
        "    dZ2 = A2 - Y\n",
        "    dW2 = np.dot(A1.T, dZ2) / n\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / n\n",
        "\n",
        "    # Hidden layer (Layer 1)\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * (A1 > 0)\n",
        "    dW1 = np.dot(X.T, dZ1) / n\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / n\n",
        "\n",
        "    gradients = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
        "\n",
        "    return gradients\n",
        "\n",
        "\n",
        "# Gradient Descent\n",
        "\n",
        "def update_parameters(W1, b1, W2, b2, gradients, learning_rate):\n",
        "    W1 -= learning_rate * gradients[\"dW1\"]\n",
        "    b1 -= learning_rate * gradients[\"db1\"]\n",
        "    W2 -= learning_rate * gradients[\"dW2\"]\n",
        "    b2 -= learning_rate * gradients[\"db2\"]\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Showdown Training Function\n",
        "def train(X_train, Y_train, epochs, learning_rate, batch_size):\n",
        "    input_size = X_train.shape[1]   # 784\n",
        "    output_size = Y_train.shape[1]  # 10\n",
        "    hidden_size = 128\n",
        "\n",
        "    # Initialize parameters\n",
        "    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "\n",
        "        # Mini-batch gradient descent\n",
        "        for i in range(0, X_train.shape[0], batch_size):\n",
        "            X_batch = X_train[i:i+batch_size]\n",
        "            Y_batch = Y_train[i:i+batch_size]\n",
        "\n",
        "            # Forward propagation\n",
        "            A1, A2 = forward_prop(X_batch, W1, b1, W2, b2)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = compute_loss(A2, Y_batch)\n",
        "            epoch_loss += loss\n",
        "\n",
        "            # Backpropagation\n",
        "            gradients = backward_prop(X_batch, Y_batch, A1, A2, W1, W2)\n",
        "\n",
        "            # Update parameters\n",
        "            W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, gradients, learning_rate)\n",
        "\n",
        "        # Print the loss after 10th cycle\n",
        "        if epoch % 10 == 0:\n",
        "          epoch_loss /= (X_train.shape[0] // batch_size)\n",
        "          print(f\"Epoch {epoch + 1}, Loss: {epoch_loss}\")\n",
        "\n",
        "\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "\n",
        "W1, b1, W2, b2 = train(x_train, y_train, epochs=100, learning_rate=0.1, batch_size=128)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bm3Yz0Ta0j5-",
        "outputId": "160b4fab-b8e6-4b22-bf4d-a4c9fbc5191a"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7621555852758589\n",
            "Epoch 11, Loss: 0.1085190529670415\n",
            "Epoch 21, Loss: 0.05917797491991317\n",
            "Epoch 31, Loss: 0.03796906332325583\n",
            "Epoch 41, Loss: 0.02588089469058749\n",
            "Epoch 51, Loss: 0.018355095971199553\n",
            "Epoch 61, Loss: 0.013445087197368744\n",
            "Epoch 71, Loss: 0.01017335552564088\n",
            "Epoch 81, Loss: 0.007903629135833482\n",
            "Epoch 91, Loss: 0.006295570847423308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, W1, b1, W2, b2):\n",
        "    _, A2 = forward_prop(X, W1, b1, W2, b2)\n",
        "    return np.argmax(A2, axis=1)\n",
        "\n",
        "y_pred = predict(x_test, W1, b1, W2, b2)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "accuracy = np.mean(y_pred == y_true)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clPgSzHF3JCp",
        "outputId": "b3e0400b-4311-4641-a37a-a19584793a91"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 97.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2HOehhteILd9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}